version: '3.8'

services:
  frontend:
    image: chattermate/frontend:latest
    ports:
      - "80:80"
    env_file:
      - ./frontend/.env
    depends_on:
      - backend
    networks:
      - chattermate-network
    restart: unless-stopped

  backend:
    image: chattermate/backend:latest
    ports:
      - "8000:8000"
    environment:
      - REDIS_ENABLED=true
      - REDIS_URL=redis://redis:6379/0
      - WORKERS=1
      - TIMEOUT=120
      - LOG_LEVEL=info
      # HuggingFace caching configuration
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers
      - SENTENCE_TRANSFORMERS_HOME=/app/.cache/huggingface/sentence_transformers
      - HF_HUB_CACHE=/app/.cache/huggingface/hub
      - HF_HUB_DISABLE_TELEMETRY=1
      # PyTorch configuration
      - TORCH_HOME=/app/.cache/torch
      - PYTORCH_TRANSFORMERS_CACHE=/app/.cache/pytorch_transformers
      # Reduce parallelism to avoid conflicts
      - EMBEDDING_MAX_WORKERS=4
      - KB_MAX_WORKERS=4
      # Enable safe embedding mode for Docker
      - EMBEDDING_SINGLE_THREADED=true
      - EMBEDDING_SEQUENTIAL_FALLBACK=true
    env_file:
      - ./backend/.env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - chattermate-network
    restart: unless-stopped
    volumes:
      - backend_data:/app/uploads
      - temp_data:/app/temp
      - ./config:/app/config
      # Add caching volumes for models
      - huggingface_cache:/app/.cache/huggingface
      - torch_cache:/app/.cache/torch
    healthcheck:
      test: ["CMD", "curl", "-X", "GET", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  knowledge_processor:
    image: chattermate/backend:latest
    command: python -m app.workers.knowledge_processor
    environment:
      - REDIS_ENABLED=true
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=info
      - PYTHONUNBUFFERED=1
      # HuggingFace caching configuration (shared with backend)
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers
      - SENTENCE_TRANSFORMERS_HOME=/app/.cache/huggingface/sentence_transformers
      - HF_HUB_CACHE=/app/.cache/huggingface/hub
      - HF_HUB_DISABLE_TELEMETRY=1
      # PyTorch configuration (shared with backend)
      - TORCH_HOME=/app/.cache/torch
      - PYTORCH_TRANSFORMERS_CACHE=/app/.cache/pytorch_transformers
      # Reduce parallelism to avoid conflicts
      - EMBEDDING_MAX_WORKERS=2
      - KB_MAX_WORKERS=2
      # Enable safe embedding mode for Docker
      - EMBEDDING_SINGLE_THREADED=true
      - EMBEDDING_SEQUENTIAL_FALLBACK=true
    env_file:
      - ./backend/.env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - chattermate-network
    restart: always
    volumes:
      - backend_data:/app/uploads
      - temp_data:/app/temp
      - ./config:/app/config
      # Share the same caching volumes with backend
      - huggingface_cache:/app/.cache/huggingface
      - torch_cache:/app/.cache/torch
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  db:
    build:
      context: .
      dockerfile: Dockerfile.postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=chattermate
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - chattermate-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - chattermate-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru

networks:
  chattermate-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  backend_data:
  temp_data:
  # Add new volumes for model caching
  huggingface_cache:
  torch_cache: